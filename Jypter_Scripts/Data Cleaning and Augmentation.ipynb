{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-02T19:31:54.095440Z",
     "start_time": "2020-07-02T19:31:54.091656Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import datetime\n",
    "import re\n",
    "import os, os.path\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Save to Single File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Lets change our directory to the training data. Then go through all folders and files, appending all the data to a single data frame. Finally export that dataframe to a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-02T14:08:47.239042Z",
     "start_time": "2020-07-02T14:08:47.231253Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/app/data/Training_Data\n"
     ]
    }
   ],
   "source": [
    "cd ../Training_Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-02T13:25:52.418392Z",
     "start_time": "2020-07-02T13:25:48.248787Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "folders = [\"fist_pump\",\"single_wave\",\"speed_mode\",\"random_motion\"]\n",
    "files =[]\n",
    "completedf = pd.DataFrame(columns=['gesture','acceleration'])\n",
    "for idx1,folder in enumerate(folders):\n",
    "    files = os.listdir(folder)\n",
    "    for idx2,file in enumerate(files):\n",
    "        df_temp = pd.read_csv(folder+'/'+file)\n",
    "        #print(df_temp[['Acc_X','Acc_Y','Acc_Z','Gyro_X', 'Gyro_Y', 'Gyro_Z']].to_numpy())\n",
    "        x=df_temp[['Acc_X','Acc_Y','Acc_Z']].to_numpy()\n",
    "        series = pd.Series(data={'gesture': folder, 'acceleration':x.tolist()})\n",
    "        df_temp2= pd.DataFrame([series])\n",
    "        completedf=pd.concat([completedf,df_temp2], ignore_index=True)  \n",
    "completedf.to_csv('complete_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Split data into Training, Validation, and Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Using Scikit Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-02T00:24:45.260205Z",
     "start_time": "2020-07-02T00:24:44.694969Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train_ratio = 0.75\n",
    "validation_ratio = 0.15\n",
    "test_ratio = 0.10\n",
    "\n",
    "# train is now 75% of the entire data set\n",
    "# the _junk suffix means that we drop that variable completely\n",
    "train_set, test_set = train_test_split(completedf, test_size=1 - train_ratio, random_state=0)\n",
    "\n",
    "# test is now 10% of the initial data set\n",
    "# validation is now 15% of the initial data set\n",
    "val_set, test_set = train_test_split(test_set, test_size=test_ratio/(test_ratio + validation_ratio), random_state=0) \n",
    "print('len of train_set: '+ str(len(train_set)))\n",
    "print('len of test_set: '+ str(len(test_set)))\n",
    "print('len of val_set: '+ str(len(val_set)))\n",
    "#print(x_train, x_val, x_test)\n",
    "train_set.to_csv('train_set.csv', index=False)\n",
    "test_set.to_csv('test_set.csv', index=False)\n",
    "val_set.to_csv('val_set.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-02T00:26:03.635964Z",
     "start_time": "2020-07-02T00:26:03.597411Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print('len of train_set Speed mode: '+ str(len(train_set.query('gesture == \"speed_mode\"'))))\n",
    "print('len of val_set Speed mode: '+ str(len(val_set.query('gesture == \"speed_mode\"'))))\n",
    "print('len of test_set Speed mode: '+ str(len(test_set.query('gesture == \"speed_mode\"'))))\n",
    "\n",
    "print('len of train_set fist_pump: '+ str(len(train_set.query('gesture == \"fist_pump\"'))))\n",
    "print('len of val_set fist_pump: '+ str(len(val_set.query('gesture == \"fist_pump\"'))))\n",
    "print('len of test_set fist_pump: '+ str(len(test_set.query('gesture == \"fist_pump\"'))))\n",
    "\n",
    "print('len of train_set single_wave: '+ str(len(train_set.query('gesture == \"single_wave\"'))))\n",
    "print('len of val_set single_wave: '+ str(len(val_set.query('gesture == \"single_wave\"'))))\n",
    "print('len of test_set single_wave: '+ str(len(test_set.query('gesture == \"single_wave\"'))))\n",
    "\n",
    "print('len of train_set random_motion: '+ str(len(train_set.query('gesture == \"random_motion\"'))))\n",
    "print('len of val_set random_motion: '+ str(len(val_set.query('gesture == \"random_motion\"'))))\n",
    "print('len of test_set random_motion: '+ str(len(test_set.query('gesture == \"random_motion\"'))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Using Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "completedf_copy= completedf.copy()\n",
    "train_set1 = completedf_copy.sample(frac=0.75, random_state=4)\n",
    "test_set1 = completedf_copy.drop(train_set1.index)\n",
    "val_set1 = test_set1.sample(frac=0.6, random_state=4)\n",
    "test_set1 = test_set1.drop(val_set.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(len(train_set),len(val_set),len(test_set))\n",
    "\n",
    "print('len of x_train Speed mode: '+ str(len(train_set.query('gesture == \"speed_mode\"'))))\n",
    "print('len of x_val Speed mode: '+ str(len(x_val.query('gesture == \"speed_mode\"'))))\n",
    "print('len of x_test Speed mode: '+ str(len(test_set.query('gesture == \"speed_mode\"'))))\n",
    "\n",
    "print('len of x_train fist_pump: '+ str(len(train_set.query('gesture == \"fist_pump\"'))))\n",
    "print('len of x_val fist_pump: '+ str(len(x_val.query('gesture == \"fist_pump\"'))))\n",
    "print('len of x_test fist_pump: '+ str(len(test_set.query('gesture == \"fist_pump\"'))))\n",
    "\n",
    "print('len of x_train single_wave: '+ str(len(train_set.query('gesture == \"single_wave\"'))))\n",
    "print('len of x_val single_wave: '+ str(len(x_val.query('gesture == \"single_wave\"'))))\n",
    "print('len of x_test single_wave: '+ str(len(test_set.query('gesture == \"single_wave\"'))))\n",
    "\n",
    "print('len of x_train random_motion: '+ str(len(train_set.query('gesture == \"random_motion\"'))))\n",
    "print('len of x_val random_motion: '+ str(len(x_val.query('gesture == \"random_motion\"'))))\n",
    "print('len of x_test random_motion: '+ str(len(test_set.query('gesture == \"random_motion\"'))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Load Data From CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-02T13:15:50.938593Z",
     "start_time": "2020-07-02T13:15:50.931968Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "cd ../Training_Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-02T14:08:52.675772Z",
     "start_time": "2020-07-02T14:08:51.534662Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train_set = pd.read_csv('train_set.csv',converters={'acceleration': eval})\n",
    "test_set = pd.read_csv('test_set.csv',converters={'acceleration': eval})\n",
    "val_set = pd.read_csv('val_set.csv',converters={'acceleration': eval})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augment Training Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why I'm augmenting my Training Data. At the time of creation, I have 168 gesture recordings. each consisting of roughly 750 samples. That is too small a number of samples to really train a model. I want to create a tensorflow lite model I can put onto my micro-controller. So how to get more gesture recordings? \n",
    "\n",
    "1) I can take more data points, but this will take me some time\n",
    "\n",
    "2) I can manipulate and save that manipulated recording as a new sample. \n",
    "\n",
    "Augmentation has another advantage over just creating more data, it helps to reduce overfitting. \n",
    "\n",
    "\n",
    "What augmentation makes sense for for my data? My data is a time series of x,y,z accelerations, of my arm moving. Thinking about myself, I can do the gesture faster and slower, more theatrical or reserved, and more cleanly or more sloppily. Luckily, I can mimic those types of changes with different algorithms. \n",
    "\n",
    "1) Increase and decrease the magnitudes of the xyz data\n",
    "\n",
    "2) Shift the data to complete faster or slower. Time stretch/shrink\n",
    "\n",
    "3) Add some noise to the data points\n",
    "\n",
    "4) Increase and decrease the the xyz data uniformly \n",
    "\n",
    "5) Shift the time window around the data, making the data start sooner or later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-02T19:25:06.665163Z",
     "start_time": "2020-07-02T19:25:03.804047Z"
    }
   },
   "outputs": [],
   "source": [
    "fract=[(3, 2), (5, 3), (2, 3), (3, 4), (9, 5), (6, 5), (4, 5)] #for creating magnitues\n",
    "#magnitude shifting \n",
    "accel_sets = train_set['acceleration'].to_numpy()\n",
    "magnitude_set = []\n",
    "magnitude_labels=[]\n",
    "magnitudedf=pd.DataFrame(columns=['gesture','acceleration'])\n",
    "for idx1, aset in enumerate(accel_sets):\n",
    "    for molecule, denominator in fract:\n",
    "        magSeries = pd.Series(data={'gesture': train_set['gesture'][idx1],\n",
    "                                    'acceleration':(np.array(aset, dtype=np.float32) * \n",
    "                                                    molecule / denominator).tolist()})\n",
    "        magnitudedf_temp=pd.DataFrame([magSeries])\n",
    "        magnitudedf=pd.concat([magnitudedf,magnitudedf_temp], ignore_index=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-02T22:16:08.720591Z",
     "start_time": "2020-07-02T22:16:03.170628Z"
    }
   },
   "outputs": [],
   "source": [
    "# Time stretch and shrink\n",
    "def time_wrapping(molecule, denominator, data):\n",
    "    \"\"\"Generate (molecule/denominator)x speed data.\"\"\"\n",
    "    tmp_data = [[0 for i in range(len(data[0]))] \n",
    "                for j in range((int(len(data) / molecule) - 1) * denominator)]\n",
    "    for i in range(int(len(data) / molecule) - 1):\n",
    "        for j in range(len(data[i])):\n",
    "            for k in range(denominator):\n",
    "                tmp_data[denominator * i +\n",
    "                         k][j] = (data[molecule * i + k][j] * (denominator - k) +\n",
    "                                  data[molecule * i + k + 1][j] * k) / denominator\n",
    "    return tmp_data\n",
    "\n",
    "timedf=pd.DataFrame(columns=['gesture','acceleration'])\n",
    "for idx1, aset in enumerate(accel_sets):\n",
    "    shiftedAccels =[]\n",
    "    for molecule, denominator in fract:\n",
    "        shiftedAccels=time_wrapping(molecule, denominator, aset)\n",
    "        timeSeries = pd.Series(data={'gesture': train_set['gesture'][idx1],\n",
    "                                     'acceleration':shiftedAccels})\n",
    "        timedf_temp=pd.DataFrame([timeSeries])\n",
    "        timedf=pd.concat([timedf,timedf_temp], ignore_index=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-02T20:14:14.026644Z",
     "start_time": "2020-07-02T20:14:10.893792Z"
    }
   },
   "outputs": [],
   "source": [
    "# Add Noise \n",
    "noisedf=pd.DataFrame(columns=['gesture','acceleration'])\n",
    "noiseyAccels =[]\n",
    "for idx1, aset in enumerate(accel_sets):\n",
    "    for t in range(5):\n",
    "        tmp_data = [[0 for i in range(len(aset[0]))] for j in range(len(aset))]\n",
    "        for q in range(len(aset)):\n",
    "            for j in range(len(aset[q])):\n",
    "                  tmp_data[q][j] = aset[q][j] + 4 * random.random()\n",
    "        noiseSeries = pd.Series(data={'gesture': train_set['gesture'][idx1],\n",
    "                                      'acceleration':tmp_data})  \n",
    "        noisedf_temp=pd.DataFrame([noiseSeries])\n",
    "        noisedf=pd.concat([noisedf,noisedf_temp], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-02T20:20:06.310982Z",
     "start_time": "2020-07-02T20:20:04.228422Z"
    }
   },
   "outputs": [],
   "source": [
    "# Shift data uniformily up or down in mag\n",
    "shiftdf=pd.DataFrame(columns=['gesture','acceleration'])\n",
    "for idx1, aset in enumerate(accel_sets):\n",
    "    for i in range(5):\n",
    "        shiftSeries = pd.Series(data={'gesture': train_set['gesture'][idx1],\n",
    "                                      'acceleration':(np.array(aset, dtype=np.float32)+\n",
    "                                                      ((random.random()- 0.5)*50)).tolist()})\n",
    "        shiftdf_temp=pd.DataFrame([shiftSeries])\n",
    "        shiftdf=pd.concat([shiftdf,shiftdf_temp], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets add all the databases together "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-02T22:31:55.447133Z",
     "start_time": "2020-07-02T22:31:55.385942Z"
    }
   },
   "outputs": [],
   "source": [
    "processedTrain_set = pd.DataFrame(columns=['gesture','acceleration'])\n",
    "processedTrain_set = pd.concat([train_set,magnitudedf, timedf, noisedf, shiftdf], \n",
    "                              ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gesture Length's Inconsistency "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see in the Arduino sketch and the subsequent python data logging sketch that I did not give it a sampling rate, or the frequency at which samples are taken. So I will have an inconsistent number of data points for each motion captured. With an unknown number of data points in each sample it can become difficult to train a machine learning model. So why did I choose to do this to my data?\n",
    "\n",
    "1. I wanted to test the consistency of my sensor and micro-controller real max data acquisition. With this knowledge I can set a fast but realistic sampling rate. \n",
    "2. The real world is filled with messy data. I wanted to gather a data set that I would have to process to use in model training. \n",
    "\n",
    "\n",
    "From the Data exploration it seems that the number of samples for each motion is roughly between 250-260 samples per second or 750-780 samples for a single gesture. \n",
    "The output of the processed data should be a consistent number, because we need a known number of inputs into our model. This number will be 750 data points. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-02T22:33:16.876037Z",
     "start_time": "2020-07-02T22:33:16.867040Z"
    }
   },
   "outputs": [],
   "source": [
    "proc_acc = processedTrain_set['acceleration'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
